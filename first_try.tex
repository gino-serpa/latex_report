\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}

\newtheorem{theorem}{Theorem}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\title{Summary of work for Dr. Willis}

\begin{document}

\maketitle

\begin{abstract}
The main objective of the support for Dr. Willis was to provide support for the clocks programs by attending the meetings, understanding the data, algorithms and approach to the analysis of the data generated in the program.
Apart from attending the meetings and having periodic meetings with Dr. Willis various tasks are described in the following sections
\end{abstract}

\section{Introduction}

The approach of the program was described in multiple documents which will not be reproduced here. Briefly the program consisted in generated a clock ensemble of various clocks (Cs, Rb, GPS) that in the case of loss of GPS could continue to provide a time reference that would not depart from "true time" within error margins described in various documents and refined during the course of the weekly meetings.

For the purposes of this report the following tasks are described:

\begin{itemize}

\item Description of the data files provided by performer and the processing that was necessary to perform in house display and analysis of the data in an efficient manner. Even though the performers have routines to analyze the data in order to have an independent assessment of the data it was necessary to develop python routines to handle the large data files

\item Kalman filters as applied to the problem of clock estimation was the subject of a bibliographic search. I will describe the resulting description after reading the literature. This provides a clear view of why the Kalman filter approach was used.


\end{itemize}


\section{Structure of the data and ingestion routines}

\subsection{The data}

The data sets generated by the clocks come in different formats with some degree of redundancy, mainly due to the large size of the data files. Data dumps were as big as gigabytes, when all variables were included or a few tens of megabytes for files with less information. Software was written to handle both kinds.

\subsection{Python library and examples}

The following are python routines written to ingest the data in an efficient manner. The actual code is included in an appendix (A)

\section{Intuition for the role of the Kalmar filter}
Central to the construction of a clock ensemble is the issue of what is the role of Kalman filters. What follows is an intuitive but explicit explanation of how Kalman filters are used to update the state of knowledge of a variable subject to periodic measurements \cite{pei2019}.

I am basically following the reference but tayloring the explanation to the case at hand. The case is build from the simplest case of a scalar variable and then generalizing to a more general case resembling the problem at hand.

Consider a variable $x$ subject to periodic measurements. For the sake of simplicity let's start with the quantity $x$ being measured with devices $1$ and $2$ yielding measurements $x_1$ and $x_2$ each measurement with a normal distribution $N(\mu,\sigma^2)$, assume for now that both devices are unbiased and uncorrelated (to be defined, for now just follow intuitively what this may mean).The problem at hand is given these two measurements what is the proper procedure to derive a better estimate for $x$.

The following result provides insight on how this problem can be tackled:

\begin{theorem}
If $x_i$ are pairwise uncorrelated, unbiased, normally distributed variables ($N(\mu,\sigma^2))$) then a linear combination $y = \sum \alpha_i x_i$ is pairwise uncorrelated to the $x_i$'s and normaly distributed with mean $\mu$ and variance $\sigma^2$:

\[ \mu = \sum \alpha_i \mu_i \]
\[ \sigma^2 = \sum \alpha_i^2 \sigma_i^2 \]

\end{theorem}


Since we want the mean of the new estimate to remain the same (unbiased estimator), in the case of two measurements, $\alpha_1$ and $\alpha_2$ should add to $1$, therefore $y = \alpha x_1 + (1-\alpha) x_2$ . We also want $\sigma^2$ to be as small as possible. Taking derivative of $\sigma^2$ with respect to $\alpha$, equating to zero and verifying that it is indeed a minimum we obtain the desired value of $\alpha$ (the Kalman gain). Defining the precission $\nu_i$ as $\frac{1}{\sigma_i^2}$ (smaller sigma larger precision) and after a bit of manipulatin we obtain:

\[
y = \dfrac{\nu_1}{\nu_1+\nu_2}x_1 + \dfrac{\nu_2}{\nu_1+\nu_2} x_2
\]

\[
\nu = \nu_1 + \nu_2
\]

This makes intuitive sense since the new precision is better than any of the initial measurements.

If there are more than one measuremnts we can treat this as generalizing the procedure above to many variables by taking the sum and using lagrange coefficients for the minimization process ${\it or}$ by succesively applying the approach to measuremnts $x_1$ and $x_2$ to produce $x_{12}$, then add $x_{12}$ to $x_3$ and so on. Both procedures yield the same result which is ocnvenient and intriguing since the latest measurement does not seem to have more weight than older measuremnts. Nevertheless the process can be summarized in a more "Kalman" notation.

\begin{theorem} Given two variables $x_1$ and $x_2$ distributed as $p_1(\mu_1,\sigma_1^2)$ and $p_2(\mu_2,\sigma_2^2)$ respectively:

\[
y(x_1,x_2) = x_1 + K(x_2-x_1)
\]
where $K$ the Kalman gain is defined as:
\[
K = \dfrac{\sigma_1^2}{\sigma_1^2+\sigma_2^2} = \dfrac{\nu_1}{\nu_1+\nu_2}
\]

the variance and precision can be expressed as:
 
\[
	\sigma_y^2 = (1-K)\sigma_1^2
\]
\[ 
	\nu_y = \nu_1 + \nu_2
\]
\end{theorem}

One interpretation of this as applied to clock properties is that measurements of the clock (clocks) are used to create a model which in turn is updated with every new measurement. Figure \ref{fig:singleparameter} illustrates he process for single parameter updating.

\begin{figure}
\includegraphics[scale=0.5]{singleparameterkalman.png}
\caption{Updating of incremental measuremnts using kallman \cite{pei2019}}
\label{fig:singleparameter}
\end{figure}

In the above case the results have as requirements that the variables be uncorrelated. In order to generalize these results is necessary to expresely state the requirement. Independence of two variables $x_1$ and $x_2$ requires that knowing the value of $x_1$ does not affect the distribution of $x_2$ i.e. $ p(x_2|x_1)=p(x_2)$, while being uncorrelated only requires $E[x_2|x_1]=E[x_2]$, a weaker requirement, that can be shown to be equivalent to  $E[(x_1-\mu_1)(x_2-\mu_2)]=0$

In the case at hand, clocks are characterized by more than one parameter, generalizing the result above $\mathbf{x}$ would represent a vector (frequency, offset, etc.), while the covariance matrix of a random variable $\mathbf{x}$ is 
$ \Sigma_\mathbf{xx} = E [ (\mathbf{x}-\boldsymbol\mu_x)   (\mathbf{x}-\boldsymbol\mu_x)^T   ] $
 using this notation now the vector variable $\mathbf{x}$ can be expressed as 
 $\mathbf{x} \sim p(\boldsymbol\mu_x,\Sigma_{\mathbf{xx}})$ 
 with a precision $\Sigma_{\mathbf{xx}^{-1}}$, in analogy to $\sigma$ and $\nu$ for the single variable case.
 
 While the above definition applies to the vector varable $\mathbf{x}$, for two vector variables $\mathbf{u}, \mathbf{v}$ the requirement for being uncorrelated become 
 $E[(\mathbf{u}-\boldsymbol\mu_u)(\mathbf{v}-\boldsymbol\mu_v)^T]=0$ 
 which is equivalent to requiring that $u_i$ and $v_j$ are all uncorrelated, or in our example that the elements describing different clocks are uncorrelated. 

All this can be summarized as:

\begin{theorem}
Let 
$\mathbf{x}_1 \sim p_1(\boldsymbol\mu_1, \Sigma_1), \ldots, 
\mathbf{x}_n \sim p_n(\boldsymbol\mu_n, \Sigma_n)$
be a set of pairwise uncorrelated vector random variables. 
Let $y = \sum A_i \mathbf{x}_i$. 

\begin{itemize}

\item
The mean and covariance matrix of $\mathbf{y}$ are:

\begin{equation}
\boldsymbol\mu_{\mathbf{y}} = \sum A_i \boldsymbol\mu_i
\end{equation}

\begin{equation}
\Sigma_\mathbf{yy} = \sum A_i \Sigma_i A_i^T
\end{equation}

\item If a random variable $\mathbf{x}_{n+1}$ is pairwise uncorrelated with 
$\mathbf{x}_1, \ldots ,\mathbf{x}_n$ 
it is uncorrelated with $\mathbf{y}$.

\end{itemize}

\end{theorem}


And the equivalent theorem for optimally fusing linear  vector estimates can be stated as

is:

\begin{theorem}
Let $x_i \sim p_i(\mu,\Sigma_i)$ be a set of pairwise uncorrelated vector random variables and
$y_A(x_1,\ldots,x_n) = \sum A_i x_i $ 
where
$\sum A_i = I$

The value of the $MSE(y_A)$ is minimized for:

\begin{equation}
A_i = \left( \sum_{j=1}^n \Sigma_j^{-1} \right)^{-1} 
      \sum \Sigma_i^{-1} \mathbf{x}_i
\end{equation}

The covariance matrix of y can be computed by

\begin{equation}
\Sigma_{yy} = \left( \sum_{j=1}^n \Sigma_j^{-1} \right)^{-1}
\end{equation}

\end{theorem}

Using the precission $N$:

\begin{equation}
y(\mathbf{x_1},\ldots,\mathbf{x}_n) = N_\mathbf{y}^{-1} \sum_{i=1}^n N_i \mathbf{x}_i
\end{equation}

\begin{equation}
N_\mathbf{y} = \sum_{j=1}^n N_j
\end{equation}

Finally in the language of Kalman filters for the case of 2 vector variables:

\begin{equation}
\mathbf{x}_1 \sim p_1(\boldsymbol\mu_1, \Sigma_1), \,
 \mathbf{x}_2 \sim p_2(\boldsymbol\mu_2, \Sigma_2)
\end{equation}


\begin{equation}
K = \Sigma_1 (\Sigma_1 + \Sigma_2)^{-1} = (N_1 + N_2)^{-1} N_2
\end{equation}


\begin{equation}
\mathbf{y} (\mathbf{x}_1, \mathbf{x}_2) = \mathbf{x}_1 + K (\mathbf{x}_2 - \mathbf{x}_1)
\end{equation}


\begin{equation}
\Sigma \mathbf{yy} = (I-K)\Sigma_1 \, {\it or} \, N_\mathbf{y} = N_1 + N_2
\end{equation}

where Fig.{\ref{fig:singleparameter}} can similarly illustrate the succesive updating of vector variables.

\section{On the evaluation of noise in measurements}

Stochatic random noise

\appendix

\section{Python library $ {\it clock\_tools.py} $ developed to ingest and display the data}

Below is a library that was created to ingest and display the data generated by the project.
Salient points in the library is the use "chunking" for data ingestion, cache decorators, and pickling the data already ingested. 

\begin{itemize}
\item  Chunking was in order to ingest the large amount of data. 
This reduced the ingestion time significantly and also avoided crashing the system due to lack of memory.

\item Cache decorators were used to allow function to retain the last thousand called values (${\it @lru\_cache(maxsize=1000)}$)  since many of the calls were repeats this shortened the execution time significantly. 

\item Saving the ingested files as pickled files (${\it pickle\_clocks(...)} $) saves time significantly when re-loading the files. This is due to the very inneficient (but general) way in which the files were stored initially. The library checks for the existence of the pickled files to save time in case they already exist. 
\end{itemize}

 
An example of how to use the library is shown in the next appendix. 

\begin{verbatim}
import pandas as pd
from astropy.time import TimeDelta, Time
from astropy import units as u
import pickle

import matplotlib.pyplot as plt

from functools import lru_cache
from pathlib import Path
import pickle

# Add a datetime64 column to the cdm_df
base_time = Time('J2000.0')

#-----------------------------------------------------------
# Functions to gather data
#-----------------------------------------------------------

@lru_cache(maxsize=1000)
def convert_epoch_to_datetime64(x):
    return (base_time + x*u.s).datetime64

def get_cdm_df(cdm_filename):
    '''
    Function to load the cdm file.
    Uses chunking to accelerate the process
    Input:
            cdm_filename
    Returns:
            df:the cdm df
    '''

    colnames = ['Epoch','Nominal','Measurement','Sigma','Clock Name']
    tp = pd.read_csv(cdm_filename,
                     sep    = "  ",
                     header = None,
                     names  = colnames,
                     engine = "python",
                    iterator = True,
                    chunksize = 100000)
    df=pd.concat(tp, ignore_index=True)

    return df


def get_clock_dfs(cdm_df):
    '''
    Given the cdm_df get the clocks df's

    Returns: clock_dfs
    '''

    # First add a datatime (legible time column)
    cdm_df['datetime64'] = cdm_df['Epoch'].apply(convert_epoch_to_datetime64)

    # Find out th enames of the clocks
    clock_names = cdm_df['Clock Name'].unique()

    # Very inneficient way to create the clocks.
    # Works but should have used groupby instead
    clock_dfs = {clock:cdm_df[cdm_df['Clock Name']==clock] \
                                 for clock in clock_names}

    # Drop the clock name
    fields_to_drop = ['Clock Name']
    for clock in clock_dfs:
        clock_dfs[clock] = clock_dfs[clock].drop(fields_to_drop, axis=1)

    return clock_dfs


def get_eto_df(eto_filename):

    file_size = eto_filename.stat().st_size
    estimated_time = file_size*333/2.52e9
    print('Should take about ', estimated_time,' seconds', flush=True)

    # Read using Chunking
    tp = pd.read_csv(eto_filename,
                 sep    = "\s+",
                 header = None,
                 engine = "python",
                iterator = True,
                chunksize = 100000)
    eto_df = pd.concat(tp, ignore_index=True)

    # Add columns with more standard time format
    eto_df['datetime'] = eto_df[0].apply(convert_epoch_to_datetime64)
    eto_df['datetime fwrp'] = eto_df[1].apply(convert_epoch_to_datetime64)

    return eto_df


def load_pickled_clocks(pickled_files_folder='pickled files'):
    '''
    Reads the pickled clock data from the CDM file
    Input:
            pickled pickled_folder
    Output:
            clock_dfs:DIctionary with the clock data frames
    '''
    # Create a list of files to load
    p = Path(pickled_files_folder).glob('NA*.pkl')
    files = [x for x in p if x.is_file()]
    files = [str(x) for x in files]
    print(files)

    # Create the dictionary to return
    clock_dfs={}
    for file in files:
        pickle_file = file
        clock = file.split('/')[-1].strip('_cdm.pkl')
        with open(pickle_file, 'rb') as handle:
                clock_dfs[clock] = pickle.load(handle)

    return clock_dfs

def load_eto(data_set_folder, eto_filename):

    # if it is already pickled just retrieve it
    pickle_file = data_set_folder/'eto.pkl'
    if pickle_file.exists():
        eto_df = unpickle_eto_df(pickle_file)
        return eto_df
    else: # If it was not pickeld then get it from scratch :(
        print('No pickled version, create it from scratch')
        eto_filename = data_set_folder/eto_filename
        eto_df = get_eto_df(eto_filename)
        column_names ={0: 'Epoch',
              1: 'Epoch fwrp',
              2: 'Total Phase vs Ensemble',
              3: 'Phase vs Ensemble',
              4: 'frequency vs Ensemble',
              5: 'drift vs Ensemble',
              6: 'Env Harmonic 1 cos',
              7: 'Env harmonic 1 -sin',
              8: 'Env Harmonic 2 cos',
              9: 'Env Harmonic 2 -sin',
              10: 'SD white noise total phase',
              11: 'SD random walk phase',
              12: 'SD random walk frequency',
              13: 'SD random walk drift',
              14: 'SD harmonic 1 random walk',
              15: 'SD harmonic 2 random walk',
              16: 'Frequency Harmonic 1',
              17: 'Frequency Harmonic 2',
              18: 'CW ensemble total phase',
              19: 'CW ensemble phase',
              20: 'CW ensemble frequency',
              21: 'CW ensemble drift',
              22: 'Measurement weight',
              23: 'prefit residual',
              24: 'postfit residual',
              25: 'Normalized prefit residual',
              26: 'Clock Status',
              27: 'Isolation Status',
              28: 'Ensemble acceleration steer',
              29: 'Weight in steering target value',
              30: 'Clock Measurement re-referenced',
              31: 'Clock Name'}
        eto_df.rename(columns=column_names, inplace=True)

        # Pickle it so this long process doesn't have not be repeated
        pickle_eto_df(eto_df, data_set_folder/'eto.pkl')

    # Rename columns

    return eto_df

def create_eto_clock_dfs(data_folder_set, eto_df):

    # First check if there is a pkl file
    filename = data_folder_set/'eto_clock_dfs.pkl'
    if filename.exists():
        with open(filename,'rb') as handle:
            eto_clock_dfs = pickle.load(handle)
        return eto_clock_dfs

    # If there is no pkl then create the clock structures
    clock_names = list(eto_df['Clock Name'].unique())
    ## Now separate the clocks
    eto_clock_dfs = {}
    for clock in clock_names:
        eto_clock_dfs[clock] = eto_df[eto_df['Clock Name']==clock]

    # Now pickle it so next time it is faster

    with open(filename,'wb') as handle:
        pickle.dump(eto_clock_dfs, handle)

    return eto_clock_dfs

def pickle_clocks(clock_dfs):
    '''
    Pickle the clocks
    '''
    for clock in clock_dfs:
        pickle_filename = clock.split('/')[-1].strip('.cdm')
        pickle_filename = pickle_filename.replace('.','_') + '_cdm.pkl'
        with open(pickle_filename,'wb') as handle:
            pickle.dump(clock_dfs[clock], handle)
    return


def  pickle_eto_df(eto_df, filename):

    with open(filename,'wb') as handle:
        pickle.dump(eto_df,handle)

    return


def unpickle_eto_df(eto_pkl_filename):
    with open(eto_pkl_filename,'rb') as handle:
        eto_df = pickle.load(handle)
    return eto_df



#                         Plotting routines
def sample_plot(n_sample, clock_dfs, clocks_to_display):

    # Sample the clocks
    sampled_clocks = {}

    for clock in clocks_to_display:
        sampled_clocks[clock]=clock_dfs[clock].iloc[::n_sample,:]

    # Create the figure
    fig,axs = plt.subplots(2, figsize=(16,16))\

    for clock in sampled_clocks:
        datetime64 = sampled_clocks[clock]['datetime64']
        measurement = sampled_clocks[clock]['Measurement']*1e9
        sigma = sampled_clocks[clock]['Sigma']

        # Top and bottom plot
        axs[0].plot(datetime64, measurement, label=clock)
        axs[1].semilogy(datetime64, sigma)

    axs[0].grid(True)
    axs[0].set_ylabel('Measurements (ns)', fontsize=20)
    axs[0].legend()

    axs[1].set_xlabel('time ', fontsize=20)
    axs[1].set_ylabel('Sigma', fontsize=20)
    axs[1].grid(True)
    axs[1].set_title('Sigma')

    plt.show()

    return

def dates_plot(from_date, to_date, clock_dfs, clocks_to_display):
    print(from_date, ' to ', to_date)
    # Sample the clocks
    dated_clocks = {}

    for clock in clocks_to_display:
        date_filter = [ item > from_date and item<to_date for item in clock_dfs[clock]['datetime64']]
        dated_clocks[clock] = clock_dfs[clock][date_filter]

    # Create the figure
    fig,axs = plt.subplots(2, figsize=(16,16))\

    for clock in dated_clocks:
        datetime64 = dated_clocks[clock]['datetime64']
        measurement = dated_clocks[clock]['Measurement']*1e9
        sigma = dated_clocks[clock]['Sigma']

        # Top and bottom plot
        axs[0].plot(datetime64, measurement, label=clock)
        axs[1].semilogy(datetime64, sigma)

    axs[0].grid(True)
    axs[0].set_ylabel('Measurements (ns)', fontsize=20)
    axs[0].legend()

    axs[1].set_xlabel('time ', fontsize=20)
    axs[1].set_ylabel('Sigma', fontsize=20)
    axs[1].grid(True)
    axs[1].set_title('Sigma')

    plt.show()

    return

def plot_eto(eto_clock_dfs, clock_list):

    plt.figure(figsize=(16,8))
    for clock in clock_list:
        x = eto_clock_dfs[clock]['datetime']
        y = eto_clock_dfs[clock]['Phase vs Ensemble']*10e9
        plt.plot(x,y,label=clock)
        plt.grid(True)
        plt.ylabel('ns', fontsize=18)
        plt.xlabel('date/time',fontsize=18)
        plt.legend(fontsize=18)
    plt.show()

    return

\end{verbatim}

\section{Jupyter notebook example on how to use the $ {\it clock\_tools}$ library}

What follows is a python version of a Jupyter notebook that uses the python library $ {\it clock\_tools}$

\begin{verbatim}
#!/usr/bin/env python
# coding: utf-8

# In[1]:

import pandas as pd
import clock_tools as ct
import matplotlib.pyplot as plt

get_ipython().run_line_magic('matplotlib', 'inline')

# ## Load the pickled clocks

# In[2]:

clock_dfs = ct.load_pickled_clocks()

# In[3]:

len(clock_dfs)

# ## Explore the data

# In[4]:

clock_dfs['NA_Local_10Mhz_Clk_Bias'].head()

# In[5]:

for clock in clock_dfs:
    print(clock)
    print(clock_dfs[clock].describe())

# In[6]:

clock_labels = [clock for clock in clock_dfs]
clock_labels

# In[7]:

clocks_to_display = [clock_labels[1],
                     clock_labels[2],
                     clock_labels[3],
                     clock_labels[5]]

# In[8]:

ct.sample_plot(100, clock_dfs, clocks_to_display )

# In[9]:

# Now control the dates

# In[10]:

from datetime import datetime as dt

#date = '201711081750'

#dt.strptime(date, '%Y%m%d%H%M')

type(dt(2017, 11, 8, 17, 50) )

# In[11]:

ct.dates_plot(dt(2021,6,7,11),
              dt(2021,6,7,11,30),
              clock_dfs, 
              clocks_to_display)

\end{verbatim}


\begin{thebibliography}{999}

\bibitem{pei2019}
  Yan Pei, {\it et al.},
  \emph{An Elementary Introduction to Kalman Filtering}.
  asXiv:1710.04055v5 [eess.SY],
  27 Jun 2019.

\end{thebibliography}

\end{document}